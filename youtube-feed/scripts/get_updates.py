#!/usr/bin/env python3
"""
è·å–å…³æ³¨çš„ YouTube åšä¸»æœ€è¿‘æ›´æ–°
ç”¨æ³•: python get_updates.py [--days 2]
"""

import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import argparse
import json

# å…³æ³¨çš„ YouTube é¢‘é“åˆ—è¡¨ (æ¥è‡ª Zara's AI Learning Library)
# æ ¼å¼: (åç§°, channel_id, é¢‘é“URL)
CHANNELS = [
    # === AI æ•™è‚² & æŠ€æœ¯æ·±åº¦ ===
    ("Andrej Karpathy", "UCXUPKJO5MZQN11PqgIvyuvQ", "@AndrejKarpathy"),
    ("Anthropic", "UCrDwWp7EBBv4NwvScIpBDOA", "@anthropic-ai"),
    ("Lex Fridman", "UCGwuxdEeCf0TIA2RbPOj-8g", "@lexfridman"),
    
    # === AI äº§å“ & åˆ›ä¸š ===
    ("Lenny's Podcast", "UCcIXPgBDgKd5EbfWi4i5cVA", "@LennysPodcast"),
    ("Peter Yang", "UCSHZKyawb77ixDdsGog4iWA", "@PeterYangYT"),
    ("The MAD Podcast (Matt Turck)", "UCQID78IY6EOojr5RUdD47MQ", "@DataDrivenNYC"),
    ("Every", "UCXZFVVCFahewxr3est7aT7Q", "@EveryInc"),
    
    # === VC & æŠ•èµ„äºº ===
    ("Y Combinator", "UCcefcZRL2oaA_uBNeo5UOWg", "@ycombinator"),
    ("Latent Space", "UCwBTFE_6Bsb_EtmXlW2aTlg", "@LatentSpacePod"),
    ("South Park Commons", "UCnpBg7yqNauHtlNSpOl5-cg", "@southparkcommons"),
    ("No Priors", "UC4Snw5yrSDMXys31I18U3gg", "@NoPriorsPodcast"),
    ("a16z", "UCE_b6sxLv68tda7tvv5YWuA", "@a16z"),
    
    # === å¤§å‚ & ç ”ç©¶ ===
    ("Google DeepMind", "UCP7jMXSY2xbc3KCAE0MHQ-A", "@googledeepmind"),
    ("Google for Developers", "UC_x5XG1OV2P6uZZ5FSM9Ttw", "@GoogleDevelopers"),
    ("Stanford GSB", "UCjIMtrzxYc0lblGhmOgC_CA", "@stanfordgsb"),
    
    # === Vibe Coding & å·¥å…· ===
    ("Mckay Wrigley", "UCbGt-LT2R9hglFeTr6KuXkw", "@realmckaywrigley"),
    ("Tiago Forte", "UCxBcwypKK-W3GHd_RZ9FZrQ", "@TiagoForte"),
    ("The Pragmatic Engineer", "UCWG5I2nL7zyrRj6bCy5qC7A", "@ThePragmaticEngineer"),
    
    # === AI æ–°é—» & è¶‹åŠ¿ ===
    ("The AI Daily Brief", "UCIAtPXNxXPKmw-_1sYnrJzQ", "@TheAIDailyBrief"),
    ("TBPN", "UCQvWX73GQygcwXOTSf_VDVg", "@TBPNLive"),
    ("Brett Malinowski", "UCMR-rPSUI34DRQXUkvFuIUQ", "@TheBrettWay"),
]


def get_channel_feed(channel_id):
    """è·å–é¢‘é“çš„ RSS feed"""
    url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            return response.text
    except Exception as e:
        print(f"è·å– feed å¤±è´¥: {e}")
    return None


def parse_feed(xml_content, channel_name, days=2):
    """è§£æ RSS feedï¼Œè·å–æœ€è¿‘ N å¤©çš„è§†é¢‘"""
    videos = []
    cutoff_date = datetime.now() - timedelta(days=days)
    
    try:
        root = ET.fromstring(xml_content)
        ns = {'atom': 'http://www.w3.org/2005/Atom', 
              'media': 'http://search.yahoo.com/mrss/',
              'yt': 'http://www.youtube.com/xml/schemas/2015'}
        
        for entry in root.findall('atom:entry', ns):
            published = entry.find('atom:published', ns)
            if published is not None:
                pub_date = datetime.fromisoformat(published.text.replace('Z', '+00:00'))
                pub_date_naive = pub_date.replace(tzinfo=None)
                
                if pub_date_naive >= cutoff_date:
                    title = entry.find('atom:title', ns)
                    video_id = entry.find('yt:videoId', ns)
                    
                    # è·å–è§†é¢‘æè¿°ä¿¡æ¯
                    media_group = entry.find('media:group', ns)
                    description = ""
                    if media_group is not None:
                        desc_elem = media_group.find('media:description', ns)
                        if desc_elem is not None and desc_elem.text:
                            description = desc_elem.text[:1500]  # è·å–æ›´å¤šæè¿°ä»¥ç”Ÿæˆæ›´è¯¦ç»†çš„æ‘˜è¦
                    
                    videos.append({
                        'channel': channel_name,
                        'title': title.text if title is not None else 'Unknown',
                        'video_id': video_id.text if video_id is not None else '',
                        'published': pub_date_naive.strftime('%Y-%m-%d %H:%M'),
                        'url': f"https://www.youtube.com/watch?v={video_id.text}" if video_id is not None else '',
                        'description': description
                    })
    except Exception as e:
        print(f"è§£æå¤±è´¥: {e}")
    
    return videos


def get_video_details(video_id):
    """è·å–è§†é¢‘è¯¦æƒ…ï¼šæ’­æ”¾æ•°å’Œæ—¶é•¿"""
    result = {'views': None, 'duration': None}
    try:
        url = f"https://www.youtube.com/watch?v={video_id}"
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            import re
            # è·å–è§‚çœ‹æ¬¡æ•°
            match = re.search(r'"viewCount":"(\d+)"', response.text)
            if match:
                views = int(match.group(1))
                if views >= 1000000:
                    result['views'] = f"{views/1000000:.1f}M"
                elif views >= 1000:
                    result['views'] = f"{views/1000:.1f}K"
                else:
                    result['views'] = str(views)
            
            # è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
            duration_match = re.search(r'"lengthSeconds":"(\d+)"', response.text)
            if duration_match:
                seconds = int(duration_match.group(1))
                hours = seconds // 3600
                minutes = (seconds % 3600) // 60
                if hours > 0:
                    result['duration'] = f"{hours}h{minutes:02d}m"
                else:
                    result['duration'] = f"{minutes}åˆ†é’Ÿ"
    except:
        pass
    return result


def generate_summary(description, title, max_chars=400):
    """ä»æè¿°ä¸­æå–æ‘˜è¦ï¼Œé»˜è®¤ 400 å­—"""
    if not description:
        return f"å…³äºã€Œ{title}ã€çš„æœ€æ–°è§†é¢‘å†…å®¹ï¼Œç‚¹å‡»é“¾æ¥è§‚çœ‹å®Œæ•´è§†é¢‘ã€‚"
    
    # æ¸…ç†æè¿°æ–‡æœ¬
    lines = description.split('\n')
    clean_lines = []
    for line in lines:
        line = line.strip()
        # è·³è¿‡é“¾æ¥è¡Œã€è®¢é˜…æç¤ºã€ç©ºè¡Œã€æ—¶é—´æˆ³
        if not line:
            continue
        if line.startswith('http') or line.startswith('Subscribe') or line.startswith('â†’'):
            continue
        if 'subscribe' in line.lower() or 'goo.gle' in line.lower():
            continue
        if line.startswith('0:') or line.startswith('1:') or line.startswith('2:'):  # è·³è¿‡æ—¶é—´æˆ³
            continue
        if line.startswith('#') and len(line) < 30:  # è·³è¿‡çŸ­æ ‡ç­¾
            continue
        clean_lines.append(line)
    
    # åˆå¹¶æˆæ‘˜è¦
    summary = ' '.join(clean_lines)
    
    # æˆªå–åˆ°åˆé€‚é•¿åº¦ï¼Œä¿æŒå¥å­å®Œæ•´
    if len(summary) > max_chars:
        # å°è¯•åœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·å¤„æˆªæ–­
        for end_char in ['ã€‚', 'ï¼', 'ï¼Ÿ', '. ', '! ', '? ', 'â€” ', ': ']:
            pos = summary[:max_chars].rfind(end_char)
            if pos > max_chars * 0.6:  # è‡³å°‘ä¿ç•™ 60% å†…å®¹
                return summary[:pos+1]
        # å¦åˆ™ç›´æ¥æˆªæ–­
        return summary[:max_chars] + '...'
    
    return summary if summary else f"å…³äºã€Œ{title}ã€çš„æœ€æ–°è§†é¢‘å†…å®¹ï¼Œç‚¹å‡»é“¾æ¥è§‚çœ‹å®Œæ•´è§†é¢‘ã€‚"


def main():
    parser = argparse.ArgumentParser(description='è·å–å…³æ³¨çš„ YouTube åšä¸»æœ€è¿‘æ›´æ–°')
    parser.add_argument('--days', type=int, default=2, help='è·å–æœ€è¿‘ N å¤©çš„æ›´æ–° (é»˜è®¤: 2)')
    parser.add_argument('--json', action='store_true', help='ä»¥ JSON æ ¼å¼è¾“å‡º')
    parser.add_argument('--markdown', action='store_true', help='ä»¥ Markdown æ ¼å¼è¾“å‡º')
    parser.add_argument('--views', action='store_true', help='è·å–æ’­æ”¾é‡ï¼ˆä¼šå¢åŠ è¯·æ±‚æ—¶é—´ï¼‰')
    args = parser.parse_args()
    
    all_videos = []
    
    import sys
    print(f"æ­£åœ¨è·å–æœ€è¿‘ {args.days} å¤©çš„æ’­å®¢æ›´æ–°...", file=sys.stderr)
    
    for name, channel_id, handle in CHANNELS:
        feed = get_channel_feed(channel_id)
        if feed:
            videos = parse_feed(feed, name, args.days)
            all_videos.extend(videos)
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
    all_videos.sort(key=lambda x: x['published'], reverse=True)
    
    # æ·»åŠ æ‘˜è¦
    for video in all_videos:
        video['summary'] = generate_summary(video['description'], video['title'])
    
    # è·å–æ’­æ”¾é‡å’Œæ—¶é•¿ï¼ˆå¯é€‰ï¼‰
    if args.views:
        print(f"æ­£åœ¨è·å–æ’­æ”¾é‡å’Œæ—¶é•¿...", file=sys.stderr)
        import time
        for video in all_videos:
            details = get_video_details(video['video_id'])
            video['views'] = details['views'] if details['views'] else '-'
            video['duration'] = details['duration'] if details['duration'] else '-'
            time.sleep(0.3)
    
    if args.json:
        print(json.dumps(all_videos, ensure_ascii=False, indent=2))
    elif args.markdown:
        # Markdown æ ¼å¼è¾“å‡º
        if not all_videos:
            print(f"æœ€è¿‘ {args.days} å¤©æ²¡æœ‰æ–°çš„æ’­å®¢æ›´æ–°ã€‚")
            return
        
        print(f"## ğŸ“º æœ€è¿‘ {args.days} å¤©å…±æœ‰ {len(all_videos)} ä¸ªæ–°æ’­å®¢æ›´æ–°\n")
        
        for i, video in enumerate(all_videos, 1):
            date_short = video['published'].split(' ')[0][5:]  # MM-DD
            duration_str = f" | â± {video.get('duration')}" if video.get('duration') else ""
            views_str = f" | ğŸ‘ {video.get('views')}" if video.get('views') else ""
            print(f"### {i}. [{video['title']}]({video['url']})")
            print(f"**{video['channel']}** | {date_short}{duration_str}{views_str}\n")
            print(f"> {video['summary']}\n")
            print("---\n")
    else:
        # é»˜è®¤æ ¼å¼è¾“å‡º
        if not all_videos:
            print(f"æœ€è¿‘ {args.days} å¤©æ²¡æœ‰æ–°çš„æ’­å®¢æ›´æ–°ã€‚")
            return
        
        print(f"\nğŸ“º æœ€è¿‘ {args.days} å¤©å…±æœ‰ {len(all_videos)} ä¸ªæ–°æ’­å®¢æ›´æ–°ï¼š\n")
        print("=" * 70)
        
        for i, video in enumerate(all_videos, 1):
            date_short = video['published'].split(' ')[0][5:]  # MM-DD
            duration_str = f" | â± {video.get('duration')}" if video.get('duration') else ""
            views_str = f" | ğŸ‘ {video.get('views')}" if video.get('views') else ""
            print(f"\n{i}. ã€{video['channel']}ã€‘{date_short}{duration_str}{views_str}")
            print(f"   ğŸ“Œ {video['title']}")
            print(f"   ğŸ”— {video['url']}")
            print(f"   ğŸ“ {video['summary']}")
            print("-" * 70)


if __name__ == "__main__":
    main()
